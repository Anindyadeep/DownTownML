{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Graph ML Resources Author Anindyadeep Sannigrahi KICK START YOUR GRAPH ML JOURNEY WITH THESE RESOURCES One of the cool topics to study these days is `Graph Machine Learning. So here are a bunch of FREE resources that can help to kickstart the Graph ML journey. I have sorted these resources based on the degree of complexity and the time taken to finish them. There is no such constraint to finish some resource in some stipulated time. Everyone is expected to learn at their own pace. Theory | Videos \u25b6\ufe0f Understanding Graph ML with DeepFindr | LEVEL : Beginner If you just want some good intuition about how Graph Neural Networks work with fewer maths, check out this playlist : PLAYLIST . First, just go through the first 6 videos and it will give you a good understanding of the basics of Graph ML and Graph Nets. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning. Though some knowledge in machine learning or deep learning is expected. Knowledge in PyTorch is a plus. Outcomes: A basic understanding of what is Graph ML. Some practical use cases of Graph ML. How message passing works . (In a more visual way) Introduction to PyTorch Geometric , a PyTorch based library to code Graph ML models easily. PLAYLIST LINK . One Hour Great session on Graph ML by Microsoft Research | LEVEL : Beginner If you want to understand Graph Machine Learning in just 1 session, Check out this Video by Microsoft Research . Here you will get to learn about the very fundamentals of Graph Neural Networks. How do the message passing paradigms works? Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning or any Deep Learning frameworks. Though some Knowledge of machine learning or deep learning is expected. Outcomes: A basic understanding of what is Graph ML. Some practical use cases of Graph ML. How message passing works and its mathematical working. Get to know different types of GNN and how are they used for various applications. VIDEO LINK CS224W is all you need | LEVEL : Beginner to Intermediate Stanford Engineering is one of the best resources out there to study any topics of Machine Learning or Deep Learning. This playlist is very long and descriptive. But it is worth watching. It covers from the very basics of Graph theory , to how Graph Neural Networks and also it covers the practical aspects of Graph ML too. Some of that includes how to deal with Scalability in Graph Machine Learning. This playlist is rather a full course on Graph Machine Learning. So it is worth investing the time in completing this course on Graph ML. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning or any Deep Learning frameworks. Though some Knowledge of machine learning or deep learning is expected. Outcomes: The basics of Graph theory. The basics of Graph Machine Learning. Intermediate Graph ML theories like different types of GNN architectures and how to build that. Getting familiar with PyTorch Geometric library. Practical aspects like scalability in Graph ML. CS224W COURSE LINK Enter Geometric Deep Learning. | LEVEL : Intermediate to Advanced Here comes another playlist , also known as the AMMI Geometric Deep Learning course. This course deals with exploring the underlined fundamental representations of natural entities like geodesics , manifolds , mesh structures by treating those as Graphs. This is exciting and this subfield is also called Geometric Deep Learning. Legendary Graph ML researchers, Michael Bronstein (Imperial College/Twitter) Joan Bruna (NYU) Taco Cohen (Qualcomm), and Petar Veli\u010dkovi\u0107 (DeepMind) are the instructors of this course. Pre-requisites: Linear algebra. Some Geometric foundations of Linear algebra are a plus. Some mathematical fundamentals of Machine Learning. The geometric side of computer vision which include topics like 3D geometry is a plus. Outcomes: Understanding a different perspective of Machine Learning. Understanding the geometric and pure non-euclidean paradigms of fundamental natural phenomena and events. Advanced theories of Machine Learning and Graph ML. In short, the outcome would be very satisfying and would open a new door for research for the aspirants. AMMI GEOMETRIC DEEP LEARNING COURSE LINK Diving Deeper to Graph ML with Research paper walk through | LEVEL : Intermediate Reading a research paper is a great way to understand and dive deeper into some topic. So, if you want to understand some of the most popular Graph Neural Network architecture papers as well as the newer topics of Graph ML covering the geometric perspectives, then you must check out the awesome paper walkthrough series by Aleksa Gordi\u0107, (Deepmind) Pre-requisites: It's better to have some prior Knowledge in Graph Machine Learning. And stuffs like how a GNN fundamentally works would save your time by not getting confused during watching the playlist. Outcomes: 1. A better and deeper Knowledge of Graph ML. 2. Understand how to read a research paper and get insights from it. PLAYLIST LINK Theory | Books \ud83d\udcd7 Now that you have got some cool video resources, here are some books for book lovers. Also, this book provides a better in-depth understanding of Graph Machine Learning. Machine Learning with PyTorch and Scikit Learn | LEVEL : Beginner One of the best books out there for beginners in ML. This book not only covers the basics of Machine Learning, and deep learning but also contains some cool topics like Graph ML and Deep Reinforcement Learning. Chapter 18 contains the full theory of what Graph ML is, and its different use cases. It also helps you to code a simple Graph Neural Network model from scratch in PyTorch . Pre-requisites: To get started with the book in general , you absolutely do not need to have any prior knowledge of Machine Learning or Deep Learning. Even this book covers the basics of PyTorch and later PyTorch Geometric too. So if you are planning to start from the very start, this book is perfect for you. But if you specifically want to read Chapter 18 of this book, then prior Knowledge of Deep learning basics and PyTorch basics is expected. Outcomes: Access to very good compiled resources of machine learning in the form of a book. Access to different other topics of ML and DL other than Graph ML. Getting a good foundation in Graph ML. BOOK LINK Graph Machine Learning | LEVEL : Beginner Now if you want to get a hands-on book dedicated to the basics of graphs and topics like How to load a Graph dataset, visualize it using NetworkX and create Machine Learning models to operate on this dataset using Tensorflow . This book is a perfect pick. Also, this book has some better visualizations to make readers understand the topics. So definitely check this out, if you are more aligned with Tensorflow. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning. Though some prior Knowledge of machine learning or deep learning and some Knowledge of how to use Tensorflow will give you an edge and will save your time. Outcomes: Getting a good foundation in Graph ML. You will be able to use some popular libraries like TensorFlow , ScikitLearn , NetworkX etc. Able to visualize Graph datasets and also apply Graph ML models to do different types of tasks. BOOK LINK Graph Neural Networks: Foundations and Applications | LEVEL : Intermediate This book provides you with a more formal way to teach Graph Machine Learning in general and how Graph Neural Networks in particular. It teaches what Representation Learning in general is. And also provides a great depth of how representation learning is used in the context of images, sound, texts, and graphs. From very popular topics to very newer and untouched topics in graph ML, this book covers it all. The book has 700+ pages. But it is not required to finish the book at once. But reading this book is surely worth it when it comes to understanding different Graph ML topics in depth. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning. Though Knowledge in Deep Learning and how Different types of Neural Network works is a plus. Also, some mathematical foundations like some common concepts of Linear algebra and calculus will save time. Outcomes: You will be in a position to explore very advanced research topics in Graph Machine Learning. A very strong theoretical foundation is guaranteed. BOOK LINK Graph Representation Learning | LEVEL : Intermediate to Advanced This is also a famous book related to representation learning and the theory behind representation learning and Graph ML. It also covers some graph theories. This book is small but is a bit advanced. So not recommended for beginners. This book covers some advanced concepts like Spectral Graph theory. But if you have some prior knowledge of ML and some foundational mathematical concepts, then this book would be a great resource to revise. Pre-requisites: Linear algebra. Some Geometric foundations of Linear algebra are a plus. Knowledge in Signal Theory and Graph signal processing is a plus. Outcomes Better and strong mathematical grasp of Graph ML and representation Learning. Better and polished mathematical foundations in the context of Graph ML. BOOK LINK Practical resources with more Hands-on projects \ud83d\ude4b\u200d\u2642\ufe0f Here are some resources containing some playlists, videos, and blogs. These are more practical and more hands-on. PLAYLIST PyTorch Geometric Hands-on Workshop | LEVEL : Begineer to Intermediate This set of videos in the playlist provides theory as well as hands-on tutorials using PyTorch Geometric . It covers all the topics like: How to load Graph data using PyG How to create a GNN model using PyG How to create a custom message-passing layer and custom GNN layer Some Advanced topics of PyG Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is required. Knowledge of Graph Neural networks is a plus. PLAYLIST LINK Extended Graph ML series by DeepFindr | LEVEL : Begineer This Playlist contains a full playlist that covers the whole basics of Graph Machine Learning. It provides cool animations and also does hand on practical using PyTorch Geometric . It is worth checking out. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. Some practical use cases of Graph ML. How message passing works . (In a more visual way) Understand how to use PyTorch Geometric libraries to create GNN models to solve some graph-related problems. PLAYLIST LINK . VIDEOS 1 Hour Session on GNN using Tensorflow | LEVEL : Begineer Check out this great 1 hr session come tutorial by Petar Veli\u010dkovi\u0107 , Where he provides some great insights on how graph machine learning works and also does a hands-on session using TensorFlow . So TensorFlow lovers, this video is a perfect fit for you. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. How to code simple GNNs using TensorFlow. VIDEO LINK BLOGS Getting started with GNN with NEPTUNE.ai | LEVEL : Begineer If want to understand the basics of GNNs, message passing, applications of GNNs, and also code a simple GNN with bare PyTorch, then this blog is all you need. One of the most compact blogs that cover all the above topics. This blog is also good for beginners if they want to get a small glimpse of Graph ML in a short period. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. How to code simple GNN using just PyTorch. Understand the introductory math of message passing. Understand the different applications of Graph ML and GNNs. BLOG LINK Hand's on Tutorial on PyTorch Geometric Blog | LEVEL : Begineer This is a great blog for starters who want to enter the Graph ML space. This two-part blog covers the very basics of Graph Machine learning concepts and also implements those concepts on PyTorch using a real-world dataset. The blog is interactive and it sure will be worth it to check that out. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. How to code simple GNNs using PyTorch Geometric BLOG PART 1 | BLOG PART 2 Understand the nitty gritty of Graph Attention Network | LEVEL : Intermediate This blog is provided by the Deep Learning group of the Indian Institute of Technology Roorkee (IITR). This blog provides some great insights about the very basic difference between the two most popular architectures viz: Graph Convolutional Network (GCN) and Graph Attention Network (GAT) . This blog also provides an in-depth tutorial of Graph Attention Network and also provides the inner working through PyTorch Code. Readers can gain some great insights and their foundations polished in the working of GATs. Prior Knowledge in Machine Learning / Deep Learning. Prior Knowledge in Graph Machine Learning. BLOG LINK DISTILL.PUB | LEVEL : Begineer to Intermediate One of the most interactive blogs that cover the topic of Graph Machine Learning. Though some parts cover the spectral graph machine learning concepts (in part 2), which require some math concepts. However, readers not aware of those concepts can just only read this blog to visualize the different representations and animations to get more intuition. Pre-requisites: Prior Knowledge in Machine Learning / Deep Learning. Some concepts of Linear Algebra like Laplacian matrix, diagonalization, etc. Outcomes Readers will get to build great visual intuition about what Graph Machine Learning is and about its different applications. BLOG LINK THE AI SUMMER | LEVEL : Begineer to Intermediate THE AI SUMMER , is one of the best new age Machine Learning blogs that not only covers Graph Machine Learning but other Hot topics like Transformers, ViTs, Multimodal learning, etc. The blog contains a subsection dedicated to Graph Machine Learning. The blog explains Graph ML and the working of GNNs along with its code in a very compact form. This source is very much useful to get some more added information regarding Graph Machine Learning. Pre-requisites: Prior Knowledge in Machine Learning / Deep Learning. Some concepts of Graph ML is a plus. BLOG LINK CS224W Notes at WANDB.ai | LEVEL : Intermediate to Advanced Another great resource of Graph Machine Learning. These are the notes of the popular CS224W course. This set of blogs covers the basics of Graph theory along with some theories of topology and network science. Further, it covers the foundational theories of Graph Neural networks and message passing. This blog is more like revision notes, so the readers should have some prior knowledge of Machine Learning and Graph Machine Learning. Pre-requisites: Prior Knowledge in Machine Learning / Deep Learning. Some concepts of Graph ML is a plus. Completion of CS224W provides a great edge. Outcomes Readers will have more in-depth clear Knowledge of foundational topics of the graph, topology, and network science. Also a great foundation of the inner nitty-gritty working of Graph Neural Networks. Knowledge of different Graph ML tasks and working with different types of GNN architectures. BLOG LINK Some List of Open Source Library especially made for Graph Machine Learning \ud83c\udf31 Name Based on Github Link Documentation Link PyTorch Geometric PyTorch LINK LINK Deep Graph Library TensorFlow and PyTorch LINK LINK Graph Nets Library TensorFlow LINK N/A Spektral TensorFlow Keras LINK LINK Jraph JAX LINK N/A Some People and Youtube Channels to Follow for Graph ML content \ud83c\udf31 NOTE: The Following ordering is done on Random Fashion Name Youtube Twitter Linkedin Aleksa Gordi\u0107 LINK LINK LINK Michael Bronstein LINK LINK LINK Petar Veli\u010dkovi\u0107 LINK LINK LINK DeepFindr LINK N/A N/A Zak Host LINK LINK LINK Letitia Parcalabescu LINK LINK LINK Jurij Leskovec LINK LINK LINK","title":"Graph ML Resources"},{"location":"#graph-ml-resources","text":"Author Anindyadeep Sannigrahi KICK START YOUR GRAPH ML JOURNEY WITH THESE RESOURCES One of the cool topics to study these days is `Graph Machine Learning. So here are a bunch of FREE resources that can help to kickstart the Graph ML journey. I have sorted these resources based on the degree of complexity and the time taken to finish them. There is no such constraint to finish some resource in some stipulated time. Everyone is expected to learn at their own pace.","title":"Graph ML Resources"},{"location":"#theory-videos","text":"","title":"Theory | Videos \u25b6\ufe0f"},{"location":"#understanding-graph-ml-with-deepfindr-level-beginner","text":"If you just want some good intuition about how Graph Neural Networks work with fewer maths, check out this playlist : PLAYLIST . First, just go through the first 6 videos and it will give you a good understanding of the basics of Graph ML and Graph Nets. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning. Though some knowledge in machine learning or deep learning is expected. Knowledge in PyTorch is a plus. Outcomes: A basic understanding of what is Graph ML. Some practical use cases of Graph ML. How message passing works . (In a more visual way) Introduction to PyTorch Geometric , a PyTorch based library to code Graph ML models easily. PLAYLIST LINK .","title":"Understanding Graph ML with DeepFindr | LEVEL : Beginner"},{"location":"#one-hour-great-session-on-graph-ml-by-microsoft-research-level-beginner","text":"If you want to understand Graph Machine Learning in just 1 session, Check out this Video by Microsoft Research . Here you will get to learn about the very fundamentals of Graph Neural Networks. How do the message passing paradigms works? Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning or any Deep Learning frameworks. Though some Knowledge of machine learning or deep learning is expected. Outcomes: A basic understanding of what is Graph ML. Some practical use cases of Graph ML. How message passing works and its mathematical working. Get to know different types of GNN and how are they used for various applications. VIDEO LINK","title":"One Hour Great session on Graph ML by Microsoft Research | LEVEL : Beginner"},{"location":"#cs224w-is-all-you-need-level-beginner-to-intermediate","text":"Stanford Engineering is one of the best resources out there to study any topics of Machine Learning or Deep Learning. This playlist is very long and descriptive. But it is worth watching. It covers from the very basics of Graph theory , to how Graph Neural Networks and also it covers the practical aspects of Graph ML too. Some of that includes how to deal with Scalability in Graph Machine Learning. This playlist is rather a full course on Graph Machine Learning. So it is worth investing the time in completing this course on Graph ML. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning or any Deep Learning frameworks. Though some Knowledge of machine learning or deep learning is expected. Outcomes: The basics of Graph theory. The basics of Graph Machine Learning. Intermediate Graph ML theories like different types of GNN architectures and how to build that. Getting familiar with PyTorch Geometric library. Practical aspects like scalability in Graph ML. CS224W COURSE LINK","title":"CS224W is all you need | LEVEL : Beginner to Intermediate"},{"location":"#enter-geometric-deep-learning-level-intermediate-to-advanced","text":"Here comes another playlist , also known as the AMMI Geometric Deep Learning course. This course deals with exploring the underlined fundamental representations of natural entities like geodesics , manifolds , mesh structures by treating those as Graphs. This is exciting and this subfield is also called Geometric Deep Learning. Legendary Graph ML researchers, Michael Bronstein (Imperial College/Twitter) Joan Bruna (NYU) Taco Cohen (Qualcomm), and Petar Veli\u010dkovi\u0107 (DeepMind) are the instructors of this course. Pre-requisites: Linear algebra. Some Geometric foundations of Linear algebra are a plus. Some mathematical fundamentals of Machine Learning. The geometric side of computer vision which include topics like 3D geometry is a plus. Outcomes: Understanding a different perspective of Machine Learning. Understanding the geometric and pure non-euclidean paradigms of fundamental natural phenomena and events. Advanced theories of Machine Learning and Graph ML. In short, the outcome would be very satisfying and would open a new door for research for the aspirants. AMMI GEOMETRIC DEEP LEARNING COURSE LINK","title":"Enter Geometric Deep Learning. | LEVEL : Intermediate to Advanced"},{"location":"#diving-deeper-to-graph-ml-with-research-paper-walk-through-level-intermediate","text":"Reading a research paper is a great way to understand and dive deeper into some topic. So, if you want to understand some of the most popular Graph Neural Network architecture papers as well as the newer topics of Graph ML covering the geometric perspectives, then you must check out the awesome paper walkthrough series by Aleksa Gordi\u0107, (Deepmind) Pre-requisites: It's better to have some prior Knowledge in Graph Machine Learning. And stuffs like how a GNN fundamentally works would save your time by not getting confused during watching the playlist. Outcomes: 1. A better and deeper Knowledge of Graph ML. 2. Understand how to read a research paper and get insights from it. PLAYLIST LINK","title":"Diving Deeper to Graph ML with Research paper walk through | LEVEL : Intermediate"},{"location":"#theory-books","text":"Now that you have got some cool video resources, here are some books for book lovers. Also, this book provides a better in-depth understanding of Graph Machine Learning.","title":"Theory | Books \ud83d\udcd7"},{"location":"#machine-learning-with-pytorch-and-scikit-learn-level-beginner","text":"One of the best books out there for beginners in ML. This book not only covers the basics of Machine Learning, and deep learning but also contains some cool topics like Graph ML and Deep Reinforcement Learning. Chapter 18 contains the full theory of what Graph ML is, and its different use cases. It also helps you to code a simple Graph Neural Network model from scratch in PyTorch . Pre-requisites: To get started with the book in general , you absolutely do not need to have any prior knowledge of Machine Learning or Deep Learning. Even this book covers the basics of PyTorch and later PyTorch Geometric too. So if you are planning to start from the very start, this book is perfect for you. But if you specifically want to read Chapter 18 of this book, then prior Knowledge of Deep learning basics and PyTorch basics is expected. Outcomes: Access to very good compiled resources of machine learning in the form of a book. Access to different other topics of ML and DL other than Graph ML. Getting a good foundation in Graph ML. BOOK LINK","title":"Machine Learning with PyTorch and Scikit Learn | LEVEL : Beginner"},{"location":"#graph-machine-learning-level-beginner","text":"Now if you want to get a hands-on book dedicated to the basics of graphs and topics like How to load a Graph dataset, visualize it using NetworkX and create Machine Learning models to operate on this dataset using Tensorflow . This book is a perfect pick. Also, this book has some better visualizations to make readers understand the topics. So definitely check this out, if you are more aligned with Tensorflow. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning. Though some prior Knowledge of machine learning or deep learning and some Knowledge of how to use Tensorflow will give you an edge and will save your time. Outcomes: Getting a good foundation in Graph ML. You will be able to use some popular libraries like TensorFlow , ScikitLearn , NetworkX etc. Able to visualize Graph datasets and also apply Graph ML models to do different types of tasks. BOOK LINK","title":"Graph Machine Learning | LEVEL : Beginner"},{"location":"#graph-neural-networks-foundations-and-applications-level-intermediate","text":"This book provides you with a more formal way to teach Graph Machine Learning in general and how Graph Neural Networks in particular. It teaches what Representation Learning in general is. And also provides a great depth of how representation learning is used in the context of images, sound, texts, and graphs. From very popular topics to very newer and untouched topics in graph ML, this book covers it all. The book has 700+ pages. But it is not required to finish the book at once. But reading this book is surely worth it when it comes to understanding different Graph ML topics in depth. Pre-requisites: You are not at all expected to have any prior knowledge of Graph Machine Learning. Though Knowledge in Deep Learning and how Different types of Neural Network works is a plus. Also, some mathematical foundations like some common concepts of Linear algebra and calculus will save time. Outcomes: You will be in a position to explore very advanced research topics in Graph Machine Learning. A very strong theoretical foundation is guaranteed. BOOK LINK","title":"Graph Neural Networks: Foundations and Applications | LEVEL : Intermediate"},{"location":"#graph-representation-learning-level-intermediate-to-advanced","text":"This is also a famous book related to representation learning and the theory behind representation learning and Graph ML. It also covers some graph theories. This book is small but is a bit advanced. So not recommended for beginners. This book covers some advanced concepts like Spectral Graph theory. But if you have some prior knowledge of ML and some foundational mathematical concepts, then this book would be a great resource to revise. Pre-requisites: Linear algebra. Some Geometric foundations of Linear algebra are a plus. Knowledge in Signal Theory and Graph signal processing is a plus. Outcomes Better and strong mathematical grasp of Graph ML and representation Learning. Better and polished mathematical foundations in the context of Graph ML. BOOK LINK","title":"Graph Representation Learning | LEVEL : Intermediate to Advanced"},{"location":"#practical-resources-with-more-hands-on-projects","text":"Here are some resources containing some playlists, videos, and blogs. These are more practical and more hands-on.","title":"Practical resources with more Hands-on projects \ud83d\ude4b\u200d\u2642\ufe0f"},{"location":"#playlist","text":"","title":"PLAYLIST"},{"location":"#pytorch-geometric-hands-on-workshop-level-begineer-to-intermediate","text":"This set of videos in the playlist provides theory as well as hands-on tutorials using PyTorch Geometric . It covers all the topics like: How to load Graph data using PyG How to create a GNN model using PyG How to create a custom message-passing layer and custom GNN layer Some Advanced topics of PyG Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is required. Knowledge of Graph Neural networks is a plus. PLAYLIST LINK","title":"PyTorch Geometric Hands-on Workshop | LEVEL : Begineer to Intermediate"},{"location":"#extended-graph-ml-series-by-deepfindr-level-begineer","text":"This Playlist contains a full playlist that covers the whole basics of Graph Machine Learning. It provides cool animations and also does hand on practical using PyTorch Geometric . It is worth checking out. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. Some practical use cases of Graph ML. How message passing works . (In a more visual way) Understand how to use PyTorch Geometric libraries to create GNN models to solve some graph-related problems. PLAYLIST LINK .","title":"Extended Graph ML series by DeepFindr | LEVEL : Begineer"},{"location":"#videos","text":"","title":"VIDEOS"},{"location":"#1-hour-session-on-gnn-using-tensorflow-level-begineer","text":"Check out this great 1 hr session come tutorial by Petar Veli\u010dkovi\u0107 , Where he provides some great insights on how graph machine learning works and also does a hands-on session using TensorFlow . So TensorFlow lovers, this video is a perfect fit for you. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. How to code simple GNNs using TensorFlow. VIDEO LINK","title":"1 Hour Session on GNN using Tensorflow | LEVEL : Begineer"},{"location":"#blogs","text":"","title":"BLOGS"},{"location":"#getting-started-with-gnn-with-neptuneai-level-begineer","text":"If want to understand the basics of GNNs, message passing, applications of GNNs, and also code a simple GNN with bare PyTorch, then this blog is all you need. One of the most compact blogs that cover all the above topics. This blog is also good for beginners if they want to get a small glimpse of Graph ML in a short period. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. How to code simple GNN using just PyTorch. Understand the introductory math of message passing. Understand the different applications of Graph ML and GNNs. BLOG LINK","title":"Getting started with GNN with NEPTUNE.ai | LEVEL : Begineer"},{"location":"#hands-on-tutorial-on-pytorch-geometric-blog-level-begineer","text":"This is a great blog for starters who want to enter the Graph ML space. This two-part blog covers the very basics of Graph Machine learning concepts and also implements those concepts on PyTorch using a real-world dataset. The blog is interactive and it sure will be worth it to check that out. Pre-requisites: Some Knowledge of Machine Learning / Deep Learning is a plus. But you are not expected to have any prior knowledge of Graph Machine Learning. Outcomes A basic understanding of what is Graph ML. How to code simple GNNs using PyTorch Geometric BLOG PART 1 | BLOG PART 2","title":"Hand's on Tutorial on PyTorch Geometric Blog | LEVEL : Begineer"},{"location":"#understand-the-nitty-gritty-of-graph-attention-network-level-intermediate","text":"This blog is provided by the Deep Learning group of the Indian Institute of Technology Roorkee (IITR). This blog provides some great insights about the very basic difference between the two most popular architectures viz: Graph Convolutional Network (GCN) and Graph Attention Network (GAT) . This blog also provides an in-depth tutorial of Graph Attention Network and also provides the inner working through PyTorch Code. Readers can gain some great insights and their foundations polished in the working of GATs. Prior Knowledge in Machine Learning / Deep Learning. Prior Knowledge in Graph Machine Learning. BLOG LINK","title":"Understand the nitty gritty of Graph Attention Network | LEVEL : Intermediate"},{"location":"#distillpub-level-begineer-to-intermediate","text":"One of the most interactive blogs that cover the topic of Graph Machine Learning. Though some parts cover the spectral graph machine learning concepts (in part 2), which require some math concepts. However, readers not aware of those concepts can just only read this blog to visualize the different representations and animations to get more intuition. Pre-requisites: Prior Knowledge in Machine Learning / Deep Learning. Some concepts of Linear Algebra like Laplacian matrix, diagonalization, etc. Outcomes Readers will get to build great visual intuition about what Graph Machine Learning is and about its different applications. BLOG LINK","title":"DISTILL.PUB | LEVEL : Begineer to Intermediate"},{"location":"#the-ai-summer-level-begineer-to-intermediate","text":"THE AI SUMMER , is one of the best new age Machine Learning blogs that not only covers Graph Machine Learning but other Hot topics like Transformers, ViTs, Multimodal learning, etc. The blog contains a subsection dedicated to Graph Machine Learning. The blog explains Graph ML and the working of GNNs along with its code in a very compact form. This source is very much useful to get some more added information regarding Graph Machine Learning. Pre-requisites: Prior Knowledge in Machine Learning / Deep Learning. Some concepts of Graph ML is a plus. BLOG LINK","title":"THE AI SUMMER | LEVEL : Begineer to Intermediate"},{"location":"#cs224w-notes-at-wandbai-level-intermediate-to-advanced","text":"Another great resource of Graph Machine Learning. These are the notes of the popular CS224W course. This set of blogs covers the basics of Graph theory along with some theories of topology and network science. Further, it covers the foundational theories of Graph Neural networks and message passing. This blog is more like revision notes, so the readers should have some prior knowledge of Machine Learning and Graph Machine Learning. Pre-requisites: Prior Knowledge in Machine Learning / Deep Learning. Some concepts of Graph ML is a plus. Completion of CS224W provides a great edge. Outcomes Readers will have more in-depth clear Knowledge of foundational topics of the graph, topology, and network science. Also a great foundation of the inner nitty-gritty working of Graph Neural Networks. Knowledge of different Graph ML tasks and working with different types of GNN architectures. BLOG LINK","title":"CS224W Notes at WANDB.ai | LEVEL : Intermediate to Advanced"},{"location":"#some-list-of-open-source-library-especially-made-for-graph-machine-learning","text":"Name Based on Github Link Documentation Link PyTorch Geometric PyTorch LINK LINK Deep Graph Library TensorFlow and PyTorch LINK LINK Graph Nets Library TensorFlow LINK N/A Spektral TensorFlow Keras LINK LINK Jraph JAX LINK N/A","title":"Some List of Open Source Library especially made for Graph Machine Learning \ud83c\udf31"},{"location":"#some-people-and-youtube-channels-to-follow-for-graph-ml-content","text":"NOTE: The Following ordering is done on Random Fashion Name Youtube Twitter Linkedin Aleksa Gordi\u0107 LINK LINK LINK Michael Bronstein LINK LINK LINK Petar Veli\u010dkovi\u0107 LINK LINK LINK DeepFindr LINK N/A N/A Zak Host LINK LINK LINK Letitia Parcalabescu LINK LINK LINK Jurij Leskovec LINK LINK LINK","title":"Some People and Youtube Channels to Follow for Graph ML content \ud83c\udf31"},{"location":"IntrotoGraphML/","tags":"Machine Learning, Deep Learning","text":"Introduction to Graph Machine Learning. Author Anindyadeep Sannigrahi This is an introductory blog post, where we will cover all the basics terminologies, to get started with GraphML. Later parts will cover details regarding each of the Graph Machine learning topics and hands-on experiences with Graph Neural networks in PyTorch Geometric or dgl. Why Graph ML is nowadays so popular? The traditional machine learning techniques, like simple linear regression, naive Bayes, decision trees, random forests, SVMs, SVRs which are nothing but a blend of statistical Inferences and computational algorithms, seemed to be very appropriate for data with very fewer complexities. But with the rising complexity of the data, w.r.t structural changes and increasing dimensionality, which can be seen in images, texts, wave-forms, graphs, etc, these kinds of algorithms started to give less promising and generalized results and facing classic problems like the curse of dimensionality, easy underfitting or overfitting of the models. So, when traditional machine learning algorithms seemed to get failed with the increasing complexity of data, that time a new subset of Machine learning called deep learning emerged out. And all deep learning algorithms from the oldest to the latest one, revolve around the foundations of the working mechanisms of Neural Networks. Based on the working Neural Networks, two more fundamental architectures, CNN for images and RNN for sequential data emerged out, and were very successful in learning and generalizing universal approximation functions for complex data like images and sequential data like texts, waveforms, etc. But here comes the twist. Till now, all the methods and types of data (images, text, etc) were falling under the category of structured data. These all types of data, like tabular data, images, texts, etc are all some kind of euclidian data. And this euclidian origin makes their learning/optimization easy and stable. But what about the Graphical data. Graphical data are so much ubiquitous, that they can be seen everywhere. Some of the examples are: The internet itself The facebook network of friends Molecular structures Our Brain (combination of millions of neurons) 3D shapes, etc. Even the images and texts or the waveforms can be seen and translated as graphical data. But those types of graphical data are kind of trivial. As those are structured, unlike general graphical data. So predicting something which is based on non-euclidean subspace is difficult for traditional deep learning or machine learning models. Because we all know that graphs do not have any certain length or shape. And how we should represent the edges. How to represent the connections. Now here anyone comes with the answer that we can use adjacency matrices. But then think of the scale of the data, we are talking about. Suppose, we have to compute giant graphs, like facebook's network of users and their friends. So for all these uncertainties, we can not use simple MLP or MLP based models to figure out optimized solutions. And so for this, a new subset of Machine Learning comes into play, which is known as Graph Machine Learning. So, what are Graphs? A graph is nothing but a collection of different nodes, which are connected with some links called edges. Mathematically we can define a graph as : G = (V, E, A) Where G represents a graph, and V represents a collection of nodes: { v1, v2 \u2026. vn} and E represents collections of edges: {e1, e2, \u2026.. em} and A represent the topological structure by defining the adjacency matrix. So this is how we generally represent a graph structure. Now we might have read in some courses like data structures, that graphs are one of the essential data structures for path-finding algorithms. This is true. But if we see, then we generally deal with nodes, which are often represented as some numbers like 1,2,3 ... or some letters like A, B, C, ... But in Machine learning, the nodes we see, do not contain some single numbers or letters. We represent each of the nodes and sometimes the edges as vectors. And so, these nodes containing some vectors within them are known to be node features. If edges are also represented as some vectors, then we define that as edge features. Now, sometimes people get confused, by considering edge features representing the connections of the graphs. This is a wrong assumption. Edge features are some kind of optional features, we use in graph ML other than node features, which helps us to learn the underlined representation more undoubtedly. But these edge features are not the representatives of the connection within the nodes. Connections are represented as the Adjacency matrix. For example, we can think of a chemical molecule as a graph. The nodes represent the atoms, and the edges represent the different types of bonds, like a single bond, double bond, etc. So for representing the types of bond, we require some features, that are represented through edge features. Edge features are not important as node features every time. Also, we will discuss more details in the later parts. Node features (Source: Research Gate ) Node features are the fundamental input for graph machine learning models. This is simply the feature vector a node of a graph is carrying. Mathematically a graph G = (V, E) where V is the set of nodes. All the nodes v that belongs to V are a d-dimensional vector. Those d-dimensional vectors are the node feature vectors. So if there are N Nodes and every node is having d-dimensional features, the input matrix X is a N x d matrix. Some simple examples might include, suppose in a molecular graph, the nodes are the atoms, and each atom may have several properties like: atomic number mass num, atomicity hybridization of the atom All these are some numerical value features, and when stacked together turns out to be a vector. Edge Features Edge features are similar to node features, but Edge features do not represent the connection in between the nodes of the graphs As mentioned in the earlier example, we already know that atoms can be considered some sort of small graphs, and these atoms have different types of bonds, which can be considered as edges of the graphs. So different kinds of properties like: Type of the bond (single, double, triple) Bond angle any other sort of chemical properties of the bonds, which have some sort of numerical/boolean values All of these when stacked upon each other form a vector of supposed m dimension. This vector formed is known as edge features of the graphs. In most of the scenarios, edge features are generally been ignored, as they are sometimes less significant than node features or not available. Adjacency matrix and Adjacency lists The adjacency matrix is the one way in which we store the connections between the graphs. An element will be 1, if there exists some kind of connection between the nodes else it will be 0. But here is a problem. Consider a giant graph, which is as big as a Facebook social network. In this case, most of the entries are zeros, making the graph a highly sparse matrix. Algorithms based on that would be highly in-efficient based on space. An alternative to the adjacency matrix is the adjacency list or coordinate format. There are different ways to represent an adjacency list. For e.g taking a group of tuples, where each tuple represents the node's source and target node connection. We can take two lists or a 2d matrix of 2 rows and m-columns, where m is the total number of valid connections, where the first row is the source and the other is the target. The figure shows the connections in the COO format of the same graph. Embeddings This is one of the most important concepts which is not only important in GraphML but also in general. We generally hear this word from the NLP field the most. But we know that embedding is everywhere. Defining an embedding is easy, it's simply we initially get a high dimensional input data (such as a word from corpora), and we make a method such that it is translated into low dimensional representations. And this representation learns the schematics of the given input, such that we get to the observer that similar kinds of input are similar to each other. For example, if we get three words {\u201cking\u201d, \u201cqueen\u201d, \u201chello\u201d} . Here king and queen have quite a similar kind of embeddings as both represent persons, elite classes, something based on history, etc . Whereas the word Hello is a greetings , which does not share the similar kind of schematics. So What are embeddings in Graphs? (Source: Research Gate ) Now if we know what embeddings are, then it's easy to know what embeddings in graphs would mean. Suppose we are given a graph, and let us consider, we have node and/or edge features. Initially, we have some kind of values of these features, and we can not find any kind of relations of different nodes by just examining those features. So we do some kind of operations on graphs, such that we transform those input features into some kind of representations, and those representations group the similar kind of nodes together. If you see in this figure (right), then we will see that initially the nodes of the graph are distributed in a random order. We apply some function f such that it captures some kind of schematics from the neighbors. After some time, we can see that nodes with similar kinds of representations stay together with lesser distances, whereas nodes with relatively less similar representations tend to stay far from each other spatially. The example picture below to the first one shows an example of the before and after learning the representation of real-world knowledge graphs. Graph Embeddings These are some kind of unified representation, that represents the whole graphs. Now, we can not acquire the features of graphs directly from the graphs. We indirectly get that from the node and/or the edge features. For sake of simplicity, suppose our graphs have only node features. Now we process these node features, by passing them into some black box, called GNN layers. And we get some more refined representation of the nodes. Now those embeddings are also nothing but a matrix (stacked vectors of different node embeddings). And we do some kind of an operation, such that we convert this (N x D) (where N = the number of nodes, D = number of features of each node), into an N-dimensional vector, such that each element of the vector represents a collective feature of each node. Altogether forming a representation of the graph. The operation done generally, in this case, is called global graph pooling . More will be covered in later blogs. ## Looking into the cycle of a GraphML problem. So now we know all the basics and components of a graph and its essential needs for machine learning. So if we see a typical Graph machine learning scenario, then it can be divided into some steps: Getting the raw data Formulating the problem as a graph problem Converting the raw data into graph data, which is a combination of nodes and meaningful edges. Using GraphMl methods to get some kind of meaningful representation of the nodes and/or edges and graph if required Doing the required level of tasks Finally the trained model is deployed into the corresponding cloud services. Application of GraphML There are innumerable applications of Graph Machine Learning. Some of them are as follows: Drug discovery. Mesh generation (2D, 3D) Molecule property detection Social circle detection Categorization of users/items Protein folding problems New-gen Recommender system Knowledge graph completions Traffic forecast Social media connections recommendations for new users. And this list goes on. We will discuss these tasks in more detail about their workings and other factors in the upcoming blogs. Conclusion So in this blog, we all have learned why these fields have emerged and grown so fast. We also learned the basics of graphs, and different properties and technical terms frequently used in Graph ML. In the next session, we will learn about different kinds of tasks of Graph ML, and different types of graphs used in Graph ML. Also in later blogs, we will deep dive into the inner working of Graph ML algorithms and make the models in PyTorch geometric or in DGL (Deep Graph Library). So stay tuned.","title":"Introduction to Graph Machine Learning"},{"location":"IntrotoGraphML/#introduction-to-graph-machine-learning","text":"Author Anindyadeep Sannigrahi This is an introductory blog post, where we will cover all the basics terminologies, to get started with GraphML. Later parts will cover details regarding each of the Graph Machine learning topics and hands-on experiences with Graph Neural networks in PyTorch Geometric or dgl.","title":"Introduction to Graph Machine Learning."},{"location":"IntrotoGraphML/#why-graph-ml-is-nowadays-so-popular","text":"The traditional machine learning techniques, like simple linear regression, naive Bayes, decision trees, random forests, SVMs, SVRs which are nothing but a blend of statistical Inferences and computational algorithms, seemed to be very appropriate for data with very fewer complexities. But with the rising complexity of the data, w.r.t structural changes and increasing dimensionality, which can be seen in images, texts, wave-forms, graphs, etc, these kinds of algorithms started to give less promising and generalized results and facing classic problems like the curse of dimensionality, easy underfitting or overfitting of the models. So, when traditional machine learning algorithms seemed to get failed with the increasing complexity of data, that time a new subset of Machine learning called deep learning emerged out. And all deep learning algorithms from the oldest to the latest one, revolve around the foundations of the working mechanisms of Neural Networks. Based on the working Neural Networks, two more fundamental architectures, CNN for images and RNN for sequential data emerged out, and were very successful in learning and generalizing universal approximation functions for complex data like images and sequential data like texts, waveforms, etc. But here comes the twist. Till now, all the methods and types of data (images, text, etc) were falling under the category of structured data. These all types of data, like tabular data, images, texts, etc are all some kind of euclidian data. And this euclidian origin makes their learning/optimization easy and stable. But what about the Graphical data. Graphical data are so much ubiquitous, that they can be seen everywhere. Some of the examples are: The internet itself The facebook network of friends Molecular structures Our Brain (combination of millions of neurons) 3D shapes, etc. Even the images and texts or the waveforms can be seen and translated as graphical data. But those types of graphical data are kind of trivial. As those are structured, unlike general graphical data. So predicting something which is based on non-euclidean subspace is difficult for traditional deep learning or machine learning models. Because we all know that graphs do not have any certain length or shape. And how we should represent the edges. How to represent the connections. Now here anyone comes with the answer that we can use adjacency matrices. But then think of the scale of the data, we are talking about. Suppose, we have to compute giant graphs, like facebook's network of users and their friends. So for all these uncertainties, we can not use simple MLP or MLP based models to figure out optimized solutions. And so for this, a new subset of Machine Learning comes into play, which is known as Graph Machine Learning.","title":"Why Graph ML is nowadays so popular?"},{"location":"IntrotoGraphML/#so-what-are-graphs","text":"A graph is nothing but a collection of different nodes, which are connected with some links called edges. Mathematically we can define a graph as : G = (V, E, A) Where G represents a graph, and V represents a collection of nodes: { v1, v2 \u2026. vn} and E represents collections of edges: {e1, e2, \u2026.. em} and A represent the topological structure by defining the adjacency matrix. So this is how we generally represent a graph structure. Now we might have read in some courses like data structures, that graphs are one of the essential data structures for path-finding algorithms. This is true. But if we see, then we generally deal with nodes, which are often represented as some numbers like 1,2,3 ... or some letters like A, B, C, ... But in Machine learning, the nodes we see, do not contain some single numbers or letters. We represent each of the nodes and sometimes the edges as vectors. And so, these nodes containing some vectors within them are known to be node features. If edges are also represented as some vectors, then we define that as edge features. Now, sometimes people get confused, by considering edge features representing the connections of the graphs. This is a wrong assumption. Edge features are some kind of optional features, we use in graph ML other than node features, which helps us to learn the underlined representation more undoubtedly. But these edge features are not the representatives of the connection within the nodes. Connections are represented as the Adjacency matrix. For example, we can think of a chemical molecule as a graph. The nodes represent the atoms, and the edges represent the different types of bonds, like a single bond, double bond, etc. So for representing the types of bond, we require some features, that are represented through edge features. Edge features are not important as node features every time. Also, we will discuss more details in the later parts.","title":"So, what are Graphs?"},{"location":"IntrotoGraphML/#node-features","text":"(Source: Research Gate ) Node features are the fundamental input for graph machine learning models. This is simply the feature vector a node of a graph is carrying. Mathematically a graph G = (V, E) where V is the set of nodes. All the nodes v that belongs to V are a d-dimensional vector. Those d-dimensional vectors are the node feature vectors. So if there are N Nodes and every node is having d-dimensional features, the input matrix X is a N x d matrix. Some simple examples might include, suppose in a molecular graph, the nodes are the atoms, and each atom may have several properties like: atomic number mass num, atomicity hybridization of the atom All these are some numerical value features, and when stacked together turns out to be a vector.","title":"Node features"},{"location":"IntrotoGraphML/#edge-features","text":"Edge features are similar to node features, but Edge features do not represent the connection in between the nodes of the graphs As mentioned in the earlier example, we already know that atoms can be considered some sort of small graphs, and these atoms have different types of bonds, which can be considered as edges of the graphs. So different kinds of properties like: Type of the bond (single, double, triple) Bond angle any other sort of chemical properties of the bonds, which have some sort of numerical/boolean values All of these when stacked upon each other form a vector of supposed m dimension. This vector formed is known as edge features of the graphs. In most of the scenarios, edge features are generally been ignored, as they are sometimes less significant than node features or not available.","title":"Edge Features"},{"location":"IntrotoGraphML/#adjacency-matrix-and-adjacency-lists","text":"The adjacency matrix is the one way in which we store the connections between the graphs. An element will be 1, if there exists some kind of connection between the nodes else it will be 0. But here is a problem. Consider a giant graph, which is as big as a Facebook social network. In this case, most of the entries are zeros, making the graph a highly sparse matrix. Algorithms based on that would be highly in-efficient based on space. An alternative to the adjacency matrix is the adjacency list or coordinate format. There are different ways to represent an adjacency list. For e.g taking a group of tuples, where each tuple represents the node's source and target node connection. We can take two lists or a 2d matrix of 2 rows and m-columns, where m is the total number of valid connections, where the first row is the source and the other is the target. The figure shows the connections in the COO format of the same graph.","title":"Adjacency matrix and Adjacency lists"},{"location":"IntrotoGraphML/#embeddings","text":"This is one of the most important concepts which is not only important in GraphML but also in general. We generally hear this word from the NLP field the most. But we know that embedding is everywhere. Defining an embedding is easy, it's simply we initially get a high dimensional input data (such as a word from corpora), and we make a method such that it is translated into low dimensional representations. And this representation learns the schematics of the given input, such that we get to the observer that similar kinds of input are similar to each other. For example, if we get three words {\u201cking\u201d, \u201cqueen\u201d, \u201chello\u201d} . Here king and queen have quite a similar kind of embeddings as both represent persons, elite classes, something based on history, etc . Whereas the word Hello is a greetings , which does not share the similar kind of schematics.","title":"Embeddings"},{"location":"IntrotoGraphML/#so-what-are-embeddings-in-graphs","text":"(Source: Research Gate ) Now if we know what embeddings are, then it's easy to know what embeddings in graphs would mean. Suppose we are given a graph, and let us consider, we have node and/or edge features. Initially, we have some kind of values of these features, and we can not find any kind of relations of different nodes by just examining those features. So we do some kind of operations on graphs, such that we transform those input features into some kind of representations, and those representations group the similar kind of nodes together. If you see in this figure (right), then we will see that initially the nodes of the graph are distributed in a random order. We apply some function f such that it captures some kind of schematics from the neighbors. After some time, we can see that nodes with similar kinds of representations stay together with lesser distances, whereas nodes with relatively less similar representations tend to stay far from each other spatially. The example picture below to the first one shows an example of the before and after learning the representation of real-world knowledge graphs.","title":"So What are embeddings in Graphs?"},{"location":"IntrotoGraphML/#graph-embeddings","text":"These are some kind of unified representation, that represents the whole graphs. Now, we can not acquire the features of graphs directly from the graphs. We indirectly get that from the node and/or the edge features. For sake of simplicity, suppose our graphs have only node features. Now we process these node features, by passing them into some black box, called GNN layers. And we get some more refined representation of the nodes. Now those embeddings are also nothing but a matrix (stacked vectors of different node embeddings). And we do some kind of an operation, such that we convert this (N x D) (where N = the number of nodes, D = number of features of each node), into an N-dimensional vector, such that each element of the vector represents a collective feature of each node. Altogether forming a representation of the graph. The operation done generally, in this case, is called global graph pooling . More will be covered in later blogs. ## Looking into the cycle of a GraphML problem. So now we know all the basics and components of a graph and its essential needs for machine learning. So if we see a typical Graph machine learning scenario, then it can be divided into some steps: Getting the raw data Formulating the problem as a graph problem Converting the raw data into graph data, which is a combination of nodes and meaningful edges. Using GraphMl methods to get some kind of meaningful representation of the nodes and/or edges and graph if required Doing the required level of tasks Finally the trained model is deployed into the corresponding cloud services.","title":"Graph Embeddings"},{"location":"IntrotoGraphML/#application-of-graphml","text":"There are innumerable applications of Graph Machine Learning. Some of them are as follows: Drug discovery. Mesh generation (2D, 3D) Molecule property detection Social circle detection Categorization of users/items Protein folding problems New-gen Recommender system Knowledge graph completions Traffic forecast Social media connections recommendations for new users. And this list goes on. We will discuss these tasks in more detail about their workings and other factors in the upcoming blogs.","title":"Application of GraphML"},{"location":"IntrotoGraphML/#conclusion","text":"So in this blog, we all have learned why these fields have emerged and grown so fast. We also learned the basics of graphs, and different properties and technical terms frequently used in Graph ML. In the next session, we will learn about different kinds of tasks of Graph ML, and different types of graphs used in Graph ML. Also in later blogs, we will deep dive into the inner working of Graph ML algorithms and make the models in PyTorch geometric or in DGL (Deep Graph Library). So stay tuned.","title":"Conclusion"}]}